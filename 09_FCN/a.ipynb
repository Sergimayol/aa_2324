{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 139811\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TinyFCN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(TinyFCN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=1, stride=1),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=1, stride=2, padding=1),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=num_classes, kernel_size=1),\n",
    "        )\n",
    "\n",
    "\n",
    "# Print the number of parameters in the model\n",
    "model = TinyFCN()\n",
    "print('Number of parameters: {}'.format(sum([p.numel() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 371073\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FCNRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNRegression, self).__init__()\n",
    "\n",
    "        # Capas convolucionales\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Capas totalmente convolucionales para regresión\n",
    "        self.fc = nn.Conv2d(256, 1, kernel_size=1)  # Última capa con un solo canal de salida para regresión\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pasar por las capas convolucionales\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "\n",
    "        # Pasar por la capa totalmente convolucional para regresión\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Crear una instancia del modelo\n",
    "model = FCNRegression()\n",
    "\n",
    "print('Number of parameters: {}'.format(sum([p.numel() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sergi/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 35312984\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MYNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MYNet, self).__init__()\n",
    "        self.model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"fcn_resnet50\", pretrained=True)\n",
    "        self.model.classifier[4] = nn.Conv2d(512, 3, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[\"out\"]\n",
    "\n",
    "\n",
    "model = MYNet()\n",
    "print(\"Number of parameters: {}\".format(sum([p.numel() for p in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
    "    model.train()\n",
    "    loss_v = 0\n",
    "\n",
    "    for batch_idx, (data, target) in (t := tqdm(enumerate(train_loader), total=len(train_loader), disable=not verbose)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0 and verbose:\n",
    "            t.set_description(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "        loss_v += loss.item()\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print(\"\\nTrain set: Average loss: {:.4f}\\n\".format(loss_v))\n",
    "    return loss_v\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch, verbose=True):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, total=len(test_loader), disable=not verbose, desc=f\"Testing: {epoch}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction=\"sum\").item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # obtener el índice de la probabilidad máxima\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "path_train = os.path.join(os.getcwd(), \"data\", \"aixi_shape_256_texture\", \"train\")\n",
    "\n",
    "files = os.listdir(path_train)\n",
    "\n",
    "img_files = [os.path.join(path_train, p) for p in files if p.endswith(\".png\")]\n",
    "label_files = [os.path.join(path_train, \"gt\", p) for p in files if p.endswith(\".png\")]\n",
    "\n",
    "\n",
    "class AIXI_Shape(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        super().__init__()\n",
    "        self.paths = images\n",
    "        self.labels = labels\n",
    "        self.len = len(self.paths)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        label_path = self.labels[index]\n",
    "        image = cv2.imread(path)\n",
    "        label = cv2.imread(label_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# image normalization\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[1.9491e-05, 4.0427e-05, 3.6870e-05], std=[0.0003, 0.0004, 0.0004]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# creació dels conjunts d'entrenament i test\n",
    "train_ds = AIXI_Shape(img_files, label_files, transform)\n",
    "# El test l'heu de crear vosaltres\n",
    "train_dl = DataLoader(train_ds, batch_size=64)\n",
    "train_dl.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "path_train = os.path.join(os.getcwd(), \"data\", \"aixi_shape_256_texture\", \"val\")\n",
    "\n",
    "test_files = os.listdir(path_train)\n",
    "\n",
    "test_img_files = [os.path.join(path_train, p) for p in files if p.endswith(\".png\")]\n",
    "test_label_files = [os.path.join(path_train, \"gt\", p) for p in files if p.endswith(\".png\")]\n",
    "\n",
    "test_ds = AIXI_Shape(test_img_files, test_label_files, transform)\n",
    "test_dl = DataLoader(test_ds, batch_size=64)\n",
    "test_dl.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sergi/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "Train Epoch: 0 [9600/10000 (96%)]\tLoss: -12.073175: 100%|██████████| 157/157 [08:12<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: -0.0775\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 0:   0%|          | 0/157 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 1, 256, 256]' is invalid for input of size 12582912",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sergi\\Documents\\GitHub\\aa_2324\\09_FCN\\a.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer \u001b[39m=\u001b[39m opt_adam \u001b[39mif\u001b[39;00m epoch \u001b[39m<\u001b[39m \u001b[39m10\u001b[39m \u001b[39melse\u001b[39;00m opt_sgd\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m train_l[epoch] \u001b[39m=\u001b[39m train(model, device, train_dl, optimizer, epoch, log_interval\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m test_l[epoch] \u001b[39m=\u001b[39m test(model, device, test_dl, epoch)\n",
      "\u001b[1;32mc:\\Users\\Sergi\\Documents\\GitHub\\aa_2324\\09_FCN\\a.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         test_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(output, target, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m         pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# obtener el índice de la probabilidad máxima\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39meq(target\u001b[39m.\u001b[39;49mview_as(pred))\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m test_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTest set: Average loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{:.0f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         test_loss, correct, \u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39mdataset), \u001b[39m100.0\u001b[39m \u001b[39m*\u001b[39m correct \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(test_loader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/09_FCN/a.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[64, 1, 256, 256]' is invalid for input of size 12582912"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(33)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device {device}\")\n",
    "\n",
    "epochs = 15\n",
    "lr = 10e-4\n",
    "\n",
    "model = MYNet().to(device)\n",
    "# Freeze the layers except the classifier\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# print(model)\n",
    "\n",
    "opt_adam = optim.Adam(model.parameters(), lr=lr)\n",
    "opt_sgd = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# Guardam el valor de pèrdua mig de cada iteració (època)\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "# Bucle d'entrenament\n",
    "for epoch in range(0, epochs):\n",
    "    optimizer = opt_adam if epoch < 10 else opt_sgd\n",
    "    train_l[epoch] = train(model, device, train_dl, optimizer, epoch, log_interval=10)\n",
    "    test_l[epoch] = test(model, device, test_dl, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Sergi/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2023-12-8 Python-3.11.5 torch-2.1.0 CUDA:0 (NVIDIA GeForce RTX 3060, 12287MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/2: 720x1280 2 persons, 2 ties\n",
      "image 2/2: 1080x810 4 persons, 1 bus\n",
      "Speed: 1504.4ms pre-process, 75.5ms inference, 4.0ms NMS per image at shape (2, 3, 640, 640)\n",
      "Saved 2 images to \u001b[1mruns\\detect\\exp6\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>confidence</th>\n",
       "      <th>class</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>742.569519</td>\n",
       "      <td>48.040802</td>\n",
       "      <td>1141.216553</td>\n",
       "      <td>716.655273</td>\n",
       "      <td>0.881913</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>442.064697</td>\n",
       "      <td>437.536072</td>\n",
       "      <td>496.798157</td>\n",
       "      <td>709.869446</td>\n",
       "      <td>0.687422</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125.246460</td>\n",
       "      <td>193.684296</td>\n",
       "      <td>712.012634</td>\n",
       "      <td>713.056274</td>\n",
       "      <td>0.638556</td>\n",
       "      <td>0</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>982.883362</td>\n",
       "      <td>308.352966</td>\n",
       "      <td>1027.358154</td>\n",
       "      <td>420.091248</td>\n",
       "      <td>0.261698</td>\n",
       "      <td>27</td>\n",
       "      <td>tie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         xmin        ymin         xmax        ymax  confidence  class    name\n",
       "0  742.569519   48.040802  1141.216553  716.655273    0.881913      0  person\n",
       "1  442.064697  437.536072   496.798157  709.869446    0.687422     27     tie\n",
       "2  125.246460  193.684296   712.012634  713.056274    0.638556      0  person\n",
       "3  982.883362  308.352966  1027.358154  420.091248    0.261698     27     tie"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)\n",
    "\n",
    "# Images\n",
    "imgs = [\n",
    "    \"https://ultralytics.com/images/zidane.jpg\",\n",
    "    \"https://ultralytics.com/images/bus.jpg\",\n",
    "]\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "\n",
    "# Results\n",
    "results.print()\n",
    "results.save()  # or .show()\n",
    "\n",
    "results.xyxy[0]  # img1 predictions (tensor)\n",
    "results.pandas().xyxy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 7763041\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "\n",
    "        ## CODER\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        ## DECODER\n",
    "        # TODO: Construeix el teu decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=features, out_channels=out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        # Aplicarem una sigmoide a la sortida de la xarxa -> TODO: Recordar el que ha dit en biel a classe\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    # Ara ja podem començar a fer coses amb cara i ulls\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "unet = UNet()\n",
    "print('Number of parameters: {}'.format(sum([p.numel() for p in unet.parameters()])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
