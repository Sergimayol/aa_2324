{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FaadnhbpCcsh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly7lrx-gCuLy"
   },
   "source": [
    "# Xarxes convolucionals\n",
    "\n",
    "L'objectiu d'avui és la creació d'una xarxa convolucional que obtengui **com a mínim igual resultat que la xarxa completament connectada implementada la setmana anterior però amb menys paràmetres**. Per poder realitzar comparacions directes emprarem el mateix conjunt de dades.\n",
    "\n",
    "Com objectius secundaris tenim:\n",
    "\n",
    "1. Aprenentatge de noves estratègies per evitar `overfitting`.\n",
    "2. Us d'un nou optimitzador.\n",
    "3. Visualització dels resultats dels filtres convolucionals.\n",
    "\n",
    "Primer de tot, com sempre, les dades:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PwSoPhjXCvV9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to data\\EMNIST\\raw\\gzip.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 561753746/561753746 [01:21<00:00, 6912126.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\EMNIST\\raw\\gzip.zip to data\\EMNIST\\raw\n"
     ]
    }
   ],
   "source": [
    "# Recordau: El label del dataset és l'índex de la llista labels. Cada posició de la llista és un codi ASCII. Podeu emprar la funció chr per fer la transformació\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),  # mitjana, desviacio tipica (precalculats)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Descarregam un dataset ja integrat en la llibreria Pytorch:\n",
    "train = datasets.EMNIST(\n",
    "    \"data\", split=\"digits\", train=True, download=True, transform=transform\n",
    ")  ## Si acabau podeu fer proves amb el split \"balanced\"\n",
    "test = datasets.EMNIST(\"data\", split=\"digits\", train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "# Transformam les dades en l'estructura necessaria per entrenar una xarxa\n",
    "train_loader = torch.utils.data.DataLoader(train, train_batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8i4Mg8KuD3r"
   },
   "source": [
    "## Definició de la xarxa\n",
    "\n",
    "### Feina a fer\n",
    "\n",
    "1. Definir la primera xarxa convolucional. A continuació teniu una llista de les capes que podeu emprar:\n",
    "\n",
    "-   `Conv2d`: Capa convolucional en 2 dimensions ([enllaç](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)). Com a paràmetres principals trobarem:\n",
    "\n",
    "    -   in_channels: canals d'entrada.\n",
    "    -   out_channels : canals de sortida.\n",
    "    -   kernel_size: mida del filtre.\n",
    "    -   stride: desplaçament del filtre. Típicament pren per valor 1.\n",
    "    -   padding: ampliació de la imatge per evitar pèrdua de dimensionalitat.\n",
    "\n",
    "-   `MaxPool2d`: Capa de max pooling ([enllaç](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)). Aquesta capa no té paràmetres entrenables. Però si:\n",
    "\n",
    "    -   kernel_size: Mida del filtre del qual es seleccionarà el màxim.\n",
    "    -   stride: desplaçament del filtre.\n",
    "\n",
    "-   `Dropout`: Dropout és un mètode de regularització (evitar `overfitting`) que aproxima l'entrenament d'un gran nombre de xarxes neuronals amb diferents arquitectures en paral·lel. Durant l'entrenament, una part de les sortides de la capa s'ignoren aleatòriament o s'abandonen. Això té l'efecte de fer que la capa sembli i es tracti com una capa amb un nombre diferent de nodes i connectivitat a la capa anterior. En efecte, cada actualització d'una capa durant l'entrenament es realitza amb una vista diferent de la capa configurada. Hem d'especificar quines capes tenen `dropout` de manera individual. Té un únic paràmetre amb valor per defecte $p=0.5$ Els valors típics d'aquest paràmetre varien entre $0.5$ i $0.8$.\n",
    "\n",
    "-   `Linear`\n",
    "\n",
    "-   `ReLU`\n",
    "\n",
    "2. Per posibilitar la visualització de les imatges passades per les capes convolucionals farem que funció `forward` tengui diverses sortides (diferents valors de `return`) un per cada capa convolucional de la xarxa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3  # Número de canales de entrada (RGB en este caso)\n",
    "out_channels = 96  # Número de filtros\n",
    "kernel_size = 11  # Tamaño del filtro (ancho y alto)\n",
    "stride = 4  # Stride\n",
    "padding = 0  # Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IQvdRDtTHdRy"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1)  # 28x28x1 -> 28x28x10\n",
    "        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # 28x28x10 -> 14x14x10\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1)  # 14x14x10 -> 14x14x20\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # 14x14x20 -> 7x7x20\n",
    "        self.fc1 = nn.Linear(7 * 7 * 20, 100)  # 7x7x20 -> 100\n",
    "        self.fc2 = nn.Linear(100, 10)  # 100 -> 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_conv1 = self.conv1(x)\n",
    "        x = F.relu(out_conv1)\n",
    "        x = self.mp1(x)\n",
    "        out_conv2 = self.conv2(x)\n",
    "        x = F.relu(out_conv2)\n",
    "        x = self.mp2(x)\n",
    "        x = x.view(-1, 7 * 7 * 20)  # flatten\n",
    "        # x = x.flatten()\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output, out_conv1, out_conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to C:\\Users\\Sergi/.cache\\torch\\hub\\v0.10.0.zip\n",
      "c:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\Sergi/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:09<00:00, 27.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load(\"pytorch/vision:v0.10.0\", \"alexnet\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6ISOL_hCk7g"
   },
   "source": [
    "## Entrenament\n",
    "\n",
    "Aquesta part, per sort, no varia massa de la setmana anterior:\n",
    "\n",
    "### Feina a fer\n",
    "\n",
    "1. Modificar la sortida de la xarxa, ara retorna diversos valors, encara que aquí només us interessa un.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "h9OLtpPzClch"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
    "    model.train()\n",
    "\n",
    "    loss_v = 0\n",
    "\n",
    "    for batch_idx, (data, target) in (t := tqdm(enumerate(train_loader), total=len(train_loader))):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _, _ = model(data)\n",
    "        loss = F.cross_entropy(output, target, reduction=\"sum\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and verbose:\n",
    "            t.set_description(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                    loss.item() / len(data),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        loss_v += loss.item()\n",
    "\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print(\"\\nTrain set: Average loss: {:.4f}\\n\".format(loss_v))\n",
    "\n",
    "    return loss_v\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, total=len(test_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, _, _ = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction=\"sum\")\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss, correct, len(test_loader.dataset), 100.0 * correct / len(test_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuació definim els paràmetres d'entrenament i el bucle principal:\n",
    "\n",
    "### Adam\n",
    "\n",
    "Aquesta setmana introduirem un nou algorisme d'optimització anomenat `Adam`. Fins ara hem emprat el descens del gradient (`SGD`).\n",
    "\n",
    "`Adam` és un algorisme d'optimització amplament emprat, tal com el descens del gradient, és iteratiu. A la literatura trobam arguments que indiquen que, tot i que Adam convergeix més ràpidament, SGD generalitza millor que Adam i, per tant, pot resultar en un rendiment final millor.\n",
    "\n",
    "[Més info](https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008)\n",
    "\n",
    "### Feina a fer:\n",
    "\n",
    "1. Mostrar el nombre de paràmetres de la xarxa i compara-ho amb el nombre de paràmetres amb la xarxa de la setmana passada). Fes la teva xarxa més petita fins que el resultat decaigui.\n",
    "\n",
    "```\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "```\n",
    "\n",
    "2. Dibuixar els gràfics de la funció de pèrdua amb les dues funcions d'optimització que coneixem: comparar `SGD` amb `ADAM`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cNIBWqAwsVSb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (mp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (mp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=980, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3750 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [236800/240000 (99%)]\tLoss: 17.726179, Average: 0.276972: 100%|██████████| 3750/3750 [03:48<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.9170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:22<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3162, Accuracy: 36430/40000 (91%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [236800/240000 (99%)]\tLoss: 8.724724, Average: 0.136324: 100%|██████████| 3750/3750 [02:54<00:00, 21.53it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.2465\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:21<00:00, 18.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2068, Accuracy: 37542/40000 (94%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [236800/240000 (99%)]\tLoss: 5.450479, Average: 0.085164: 100%|██████████| 3750/3750 [04:11<00:00, 14.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.1850\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:19<00:00, 20.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1672, Accuracy: 38041/40000 (95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [236800/240000 (99%)]\tLoss: 3.435353, Average: 0.053677: 100%|██████████| 3750/3750 [02:49<00:00, 22.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.1543\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:19<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1417, Accuracy: 38351/40000 (96%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [236800/240000 (99%)]\tLoss: 2.251691, Average: 0.035183: 100%|██████████| 3750/3750 [02:54<00:00, 21.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.1323\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:18<00:00, 21.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1225, Accuracy: 38558/40000 (96%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [236800/240000 (99%)]\tLoss: 1.578216, Average: 0.024660: 100%|██████████| 3750/3750 [02:49<00:00, 22.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.1154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:19<00:00, 20.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1075, Accuracy: 38725/40000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [236800/240000 (99%)]\tLoss: 1.184980, Average: 0.018515: 100%|██████████| 3750/3750 [03:00<00:00, 20.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.1021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:22<00:00, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0956, Accuracy: 38870/40000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [236800/240000 (99%)]\tLoss: 0.912534, Average: 0.014258: 100%|██████████| 3750/3750 [03:13<00:00, 19.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.0915\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:20<00:00, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0860, Accuracy: 38984/40000 (97%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [236800/240000 (99%)]\tLoss: 0.713598, Average: 0.011150: 100%|██████████| 3750/3750 [02:54<00:00, 21.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.0830\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:19<00:00, 20.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0782, Accuracy: 39066/40000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [236800/240000 (99%)]\tLoss: 0.567611, Average: 0.008869: 100%|██████████| 3750/3750 [03:04<00:00, 20.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.0761\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:21<00:00, 18.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0719, Accuracy: 39129/40000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [236800/240000 (99%)]\tLoss: 0.479456, Average: 0.007492: 100%|██████████| 3750/3750 [02:54<00:00, 21.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.0704\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:19<00:00, 20.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0667, Accuracy: 39196/40000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [236800/240000 (99%)]\tLoss: 0.407008, Average: 0.006359: 100%|██████████| 3750/3750 [02:57<00:00, 21.10it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.0657\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:22<00:00, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0622, Accuracy: 39249/40000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [236800/240000 (99%)]\tLoss: 0.355056, Average: 0.005548: 100%|██████████| 3750/3750 [02:51<00:00, 21.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.0616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:33<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0584, Accuracy: 39298/40000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [236800/240000 (99%)]\tLoss: 0.317678, Average: 0.004964: 100%|██████████| 3750/3750 [03:26<00:00, 18.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.0581\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:20<00:00, 19.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0552, Accuracy: 39334/40000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [236800/240000 (99%)]\tLoss: 0.289820, Average: 0.004528: 100%|██████████| 3750/3750 [04:23<00:00, 14.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train set: Average loss: 0.0551\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:33<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0524, Accuracy: 39361/40000 (98%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(33)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 15\n",
    "lr = 0.00001\n",
    "\n",
    "model = Net().to(device)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Guardam el valor de peèrdua mig de cada iteració (època)\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "# Bucle d'entrenament\n",
    "for epoch in range(0, epochs):\n",
    "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_l[epoch] = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjeMWK8cJkqN"
   },
   "source": [
    "## Resultats\n",
    "\n",
    "Aquí visualitzarem els resultats d'aprenentatge de la xarxa.\n",
    "\n",
    "### Feina a fer:\n",
    "\n",
    "1. Fer una predicció del primer _batch_ del conjunt de _test_.\n",
    "2. Visualitzar una imatge del _batch_ i posar la predicció i el groun truth com a títol de la imatge.\n",
    "3. Visualitzar el resultat de la mateixa imatge passada per tots els filtres de la primera convolució de la vostra xarxa.\n",
    "4. **Extra**: Fer la matriu de confusió de les 10 classes per poder entendre el que no estau fent bé (la xarxa no està fent bé).\n",
    "\n",
    "A tenir en compte:\n",
    "\n",
    "#### Subplots\n",
    "\n",
    "Per fer graelles d'imatges podeu empar la funció `subplots`. Més [informació](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html)\n",
    "\n",
    "#### Device\n",
    "\n",
    "Si heu emprat _GPU_ per accelerar el procés d'entrenament, els resultats que obtenim de la xarxa també seràn a la _GPU_. **Pytorch** proporciona la funció `cpu()` que retorna una còpia d'aquest objecte a la memòria de la CPU.\n",
    "\n",
    "#### Detach\n",
    "\n",
    "Per poder operar amb els resultats de la predicció emprarem la funció `detach` que retorna un nou Tensor \"separat\" del graf (xarxa) en curs.\n",
    "\n",
    "Per tant per transformar el tensor que retorna la xarxa en un array de la lliberia _Numpy_ caldria fer el següent:\n",
    "\n",
    "```\n",
    "resultat_np = resultat.detach().numpy()\n",
    "```\n",
    "\n",
    "Si a més hem executat l'entrenament en _GPU_:\n",
    "\n",
    "```\n",
    "resultat_np = resultat.cpu().detach().numpy()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SYKUppOc_4JE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 10]),\n",
       " torch.Size([100, 10, 28, 28]),\n",
       " torch.Size([100, 20, 14, 14]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generador(loader):\n",
    "    for data, target in loader:\n",
    "        yield data, target\n",
    "\n",
    "\n",
    "data, target = next(generador(test_loader))\n",
    "\n",
    "output, out_conv1, out_conv2 = model(data.to(device))\n",
    "\n",
    "output.shape, out_conv1.shape, out_conv2.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
