{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ozwAAiAyNHA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMy35hII78XT"
   },
   "source": [
    "# Transferència de coneixement\n",
    "\n",
    "L'objectiu d'avui és aprendre com podem emprar arquitectures ja existents per resoldre els nostres problemes. \n",
    "\n",
    "Com objectius secundaris tenim:\n",
    "\n",
    "1. Conèixer un nou conjunt de dades\n",
    "2. Entendre en profunditat com són dos dels models més emprats.\n",
    "3. Guardar i carregar xarxes neuronals\n",
    "\n",
    "## Dades\n",
    "\n",
    "El conjunt de dades [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) consta de 60.000 imatges en color de 32x32 pixels etiquetades en 10 classes, amb 6.000 imatges per classe. Hi ha 50.000 imatges d'entrenament i 10.000 imatges de _test_.\n",
    "\n",
    "\n",
    "Si voleu normalitzar les dades, a continuació teniu els valors ja calculats:\n",
    "\n",
    "  - mitjana: (0.4914, 0.4822, 0.4465)\n",
    "  - desviació típica: (0.247, 0.243, 0.261)\n",
    "\n",
    "Una altra funció que pot ser útil és `Resize(mida_desti)` que rep un enter com a paràmetre (la mida final).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjdicviT7-dg",
    "outputId": "2b758f54-06ba-4d2c-ae2d-f90bd36a2570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "USE_ALEXNET = True\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "img_size = (227, 227) if USE_ALEXNET else (224, 224)\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)), transforms.Resize(img_size)]\n",
    ")\n",
    "\n",
    "# Descarregam un dataset ja integrat en la llibreria Pytorch\n",
    "train_data = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(\"./data\", train=False, transform=transform)\n",
    "\n",
    "# Transformam les dades en l'estructura necessaria per entrenar una xarxa\n",
    "train_loader = torch.utils.data.DataLoader(train_data, train_batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3),\n",
       " (10000, 32, 32, 3),\n",
       " (50000, 32, 32, 3),\n",
       " (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape, test_data.data.shape, train_loader.dataset.data.shape, test_loader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dty0xrAh71Qw"
   },
   "source": [
    "## Transfer learning (Definició de la xarxa)\n",
    "\n",
    "En aquesta pràctica aplicarem la tècnica de _transfer learning_ a partir de dues de les xarxes més conegudes en el camp de visió per computador:\n",
    "\n",
    "-[**AlexNet**](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). (ImageNet Classification with Deep Convolutional Neural Network, 2012). La mida d'entrada de les imatges és de (227x227x3).Té prop de 60 milions de paràmetres entrenables.\n",
    "\n",
    "[**MobileNet v3**](https://pytorch.org/vision/main/models/mobilenetv3.html): és una arquitectura de xarxa neuronal convolucional dissenyada per a aplicacions de visió per ordinador en dispositius mòbils i sistemes encastats. Va ser desenvolupada per Google a la seva sèrie MobileNet, que se centra en l'eficiència i el rendiment de les xarxes neuronals en dispositius amb recursos limitats, com ara telèfons mòbils. Té dues versions: _small_ amb configuracions que van dels 2 als 5 milions de paràmetres; _large_ amb uns 6 milions de paràmetres. La mida d'entrada de les imatges és de (224x224x3)\n",
    "\n",
    "_Pytorch_ ens permet emprar aquestes xarxes de manera molt senzilla. [Més informació](https://pytorch.org/vision/stable/models.html).\n",
    "\n",
    "Si el model que cercam no es troba integrat dins la llibreria _Pytorch_ és bastant probable que si la trobem a Huggingface.\n",
    "\n",
    "Descarregarem AlexNet i a analitzar-la. En aquest cas no només ens baixam la seva arquitectura, també els pesos resultants de l'entrenament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xaoFxi7cygHX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to C:\\Users\\Sergi/.cache\\torch\\hub\\checkpoints\\alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [00:15<00:00, 16.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Arquitectura AlexNet\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex = models.alexnet(weights=True)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Arquitectura AlexNet\")\n",
    "print(\"-\"*50)\n",
    "alex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to C:\\Users\\Sergi/.cache\\torch\\hub\\checkpoints\\mobilenet_v3_small-047dcff4.pth\n",
      "100%|██████████| 9.83M/9.83M [00:00<00:00, 28.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Arquitectura MobileNet\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobilenet  = models.mobilenet_v3_small(weights=True)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"Arquitectura MobileNet\")\n",
    "print(\"-\"*50)\n",
    "mobilenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asepjghw2xED"
   },
   "source": [
    "Hi ha diverses maneres de realitzar la tècnica de _TransferLearning_ les dues més conegudes són:\n",
    "\n",
    " - **\"Congelar\"** els pesos de la part d'extracció de característiques (la part convolucional) i crear un nou classificador que s'adapti al nostre problema. Això implica que només entrenam una part de la xarxa.\n",
    " - **Reentrenar tota la xarxa**.\n",
    "\n",
    "Com que la nostra capacitat de càlcul és limitada, ens decantarem per la primera opció. Pensau que\n",
    "\n",
    " Per tal d'evitar el reentrenament necessitam canviar el valor de l'atribut  `requires_grad` al valor `False`. Aquest atribut és propietat de cada tensor. Podem recorrer els tensors mitjançant el següent codi:\n",
    " ```\n",
    "for param in alex.features.parameters():\n",
    "    param.requires_grad = False\n",
    " ```\n",
    " ### Feina a fer:\n",
    "\n",
    "- Bloc 1\n",
    " 1. Carregar la xarxa AlexNet i seleccionar la part d'extracció de característiques.\n",
    " 2. Definir un entorn seqüencial on implementarem el classificador de la xarxa.\n",
    " 3. Realitzar un entrenament: comparar rendiment (accuracy) i nombre de paràmetres.\n",
    " 4. Provar de guardar la vostra xarxa i tornar-la a carregar. Classificar una imatge del conjunt de test.\n",
    "- Bloc 2: Repetir el mateix procés que l'anterior amb la xarxa mobilenet v3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in alex.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in mobilenet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZrzR1-4Ny3hx"
   },
   "outputs": [],
   "source": [
    "\n",
    "base = alex if USE_ALEXNET else mobilenet\n",
    "\n",
    "my_net = nn.Sequential(base, nn.Flatten(1, -1), nn.Linear(1000, 10), nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZH80zEa8IPW"
   },
   "source": [
    "## Entrenament\n",
    "\n",
    "[shhht](https://github.com/tqdm/tqdm) si voleu canviar el resum de l'entrenament per una barra de progrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "eJiXfzTM7e8d"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True):\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    loss_v = 0\n",
    "\n",
    "    for batch_idx, (data, target) in (t:= tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\")):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target, reduction='mean') \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and verbose:\n",
    "            t.set_description('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(), loss.item()/ len(data)))\n",
    "        loss_v += loss.item()\n",
    "\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}\\n'.format(loss_v))\n",
    " \n",
    "    return loss_v\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, total=len(test_loader), desc=\"Test\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='mean') \n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    " \n",
    "  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "llV5gCGU7jIT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters  10010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/782 [00:00<?, ?it/s]c:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.050122, Average: 0.032033:   7%|▋         | 58/782 [01:05<13:35,  1.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sergi\\Documents\\GitHub\\aa_2324\\08_CNN_Transfer\\transfer.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/08_CNN_Transfer/transfer.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_l \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((epochs))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/08_CNN_Transfer/transfer.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/08_CNN_Transfer/transfer.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     train_l[epoch] \u001b[39m=\u001b[39m train(model, device, train_loader, optimizer, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/08_CNN_Transfer/transfer.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     test_l[epoch]  \u001b[39m=\u001b[39m test(model, device, test_loader)\n",
      "\u001b[1;32mc:\\Users\\Sergi\\Documents\\GitHub\\aa_2324\\08_CNN_Transfer\\transfer.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/08_CNN_Transfer/transfer.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/08_CNN_Transfer/transfer.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/08_CNN_Transfer/transfer.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/08_CNN_Transfer/transfer.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(output, target, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergi/Documents/GitHub/aa_2324/08_CNN_Transfer/transfer.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torchvision\\models\\alexnet.py:48\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m---> 48\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[0;32m     49\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     50\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Sergi\\miniconda3\\envs\\ml-notes\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(33)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    \n",
    "\n",
    "epochs = 2\n",
    "lr = 1e-3\n",
    "\n",
    "model = my_net.to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Parameters \", pytorch_total_params)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Guardam el valor de pèrdua mig de cada iteració (època)\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_l[epoch]  = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "AJzao3Z7Jlc_"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGzCAYAAADT4Tb9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA08ElEQVR4nO3deVyVZf7/8fdhx4WDKIsi7ruifoMgssKUpDQnSkczR8XM7DtaOaaOtqiUE9nmMjY6zkw6lWbptFg5lqFpKZlbVuSWaa6gZIJIIsL9+6Mf59uJRUAOyOXr+XicR3Kd677uz31x4ry57vs+2CzLsgQAAGAIt5ouAAAAoCoRbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBugFrKZrNpxowZNV1GpSxZskQ2m02HDh2qkvE++eQT2Ww2ffLJJ1UyHoDajXADlKDozbfo4eHhodDQUCUmJurYsWM1XV6JNm/erBkzZujMmTOVHuNvf/ublixZUmU11aTExET17NnTpfv49ttvNWPGjCoLaVejqnjdAr9FuAHK8OSTT+rVV1/VwoULddttt+m1115TbGyszp8/X9OlFbN582YlJSURbqrRt99+q6SkJMLNZaiK1y3wWx41XQBwJbvtttsUGRkpSbrvvvvUqFEjzZo1S6tWrdKgQYNquDrUJpZl6fz58/L19a3pUgDjsXIDVMCNN94oSTpw4IBT+549ezRw4EAFBATIx8dHkZGRWrVqlVOf/Px8JSUlqW3btvLx8VHDhg11ww03aO3atY4+PXv2LPFUSmJiolq0aFFqXTNmzNCkSZMkSS1btnScTitaUVi8eLF69eqloKAgeXt7q1OnTlqwYIHTGC1atFBaWpo2bNjg2L6olvLUXpq0tDT16tVLvr6+atq0qWbOnKnCwsIS+/73v//VjTfeqLp166p+/frq16+f0tLSLrmP8iosLNScOXPUuXNn+fj4KDg4WGPGjNFPP/3k1K9Fixa6/fbb9dlnnykqKko+Pj5q1aqVXnnlFUefJUuW6Pe//70k6eabb3bMWdF1P0VjfPjhh4qMjJSvr6/+/ve/S5LOnDmj8ePHKywsTN7e3mrTpo1mzZrlNC+HDh2SzWbT888/r0WLFql169by9vbWtddeq61btzrV+9VXXykxMVGtWrWSj4+PQkJCdO+99+rHH3906jdjxgzZbDbt27dPf/jDH2S32xUYGKgnnnhClmXpyJEjuuOOO+Tn56eQkBC98MILxeYwLy9P06dPV5s2beTt7a2wsDBNnjxZeXl5Tv1sNpvGjRund955R126dJG3t7c6d+6sNWvWONVT1usWqCxWboAKKPqh26BBA0dbWlqaevToodDQUE2ZMkV169bVm2++qYSEBP3nP//RnXfeKemXH+TJycm67777FBUVpezsbG3btk07duzQLbfccll13XXXXdq3b59ef/11zZ49W40aNZIkBQYGSpIWLFigzp0763e/+508PDz03nvv6Y9//KMKCws1duxYSdKcOXP04IMPql69enrsscckScHBwZdVe3p6um6++WZdvHjRMTeLFi0qcfXi1Vdf1YgRIxQfH69Zs2YpNzdXCxYs0A033KCdO3eWGe7Ka8yYMVqyZIlGjhyphx56SAcPHtT8+fO1c+dObdq0SZ6eno6+3333nQYOHKhRo0ZpxIgRevnll5WYmKiIiAh17txZN910kx566CHNmzdPjz76qDp27ChJjv9K0t69ezVkyBCNGTNGo0ePVvv27ZWbm6vY2FgdO3ZMY8aMUbNmzbR582ZNnTpVJ06c0Jw5c5xqXrZsmc6ePasxY8bIZrPp2Wef1V133aXvv//eUe/atWv1/fffa+TIkQoJCVFaWpoWLVqktLQ0ff7557LZbE5jDh48WB07dtQzzzyjDz74QDNnzlRAQID+/ve/q1evXpo1a5aWLl2qiRMn6tprr9VNN90k6Zdw+Lvf/U6fffaZ7r//fnXs2FFff/21Zs+erX379umdd95x2s9nn32mt956S3/84x9Vv359zZs3TwMGDNDhw4fVsGHDS75ugUqzABSzePFiS5L18ccfW6dOnbKOHDlirVy50goMDLS8vb2tI0eOOPr27t3bCg8Pt86fP+9oKywstK6//nqrbdu2jrZu3bpZ/fr1K3O/sbGxVmxsbLH2ESNGWM2bN3dqk2RNnz7d8fVzzz1nSbIOHjxYbPvc3NxibfHx8VarVq2c2jp37lzi/stTe0nGjx9vSbK2bNniaDt58qRlt9udaj179qzl7+9vjR492mn79PR0y263F2v/rfXr11uSrPXr15fa59NPP7UkWUuXLnVqX7NmTbH25s2bW5KsjRs3OtXt7e1tPfLII462FStWlLrfojHWrFnj1P7UU09ZdevWtfbt2+fUPmXKFMvd3d06fPiwZVmWdfDgQUuS1bBhQ+v06dOOfu+++64lyXrvvfccbSV9f19//fVixzB9+nRLknX//fc72i5evGg1bdrUstls1jPPPONo/+mnnyxfX19rxIgRjrZXX33VcnNzsz799FOnfS1cuNCSZG3atMnRJsny8vKyvvvuO0fbrl27LEnWX//6V0dbWa9boLI4LQWUIS4uToGBgQoLC9PAgQNVt25drVq1Sk2bNpUknT59WuvWrdOgQYN09uxZZWZmKjMzUz/++KPi4+O1f/9+x91V/v7+SktL0/79+6v9OH69UpKVlaXMzEzFxsbq+++/V1ZW1iW3r2ztq1ev1nXXXaeoqChHW2BgoIYOHerUb+3atTpz5oyGDBnimMPMzEy5u7srOjpa69evr9B+S7JixQrZ7XbdcsstTvuIiIhQvXr1iu2jU6dOjtOQRXW3b99e33//fbn32bJlS8XHxxer48Ybb1SDBg2c6oiLi1NBQYE2btzo1H/w4MFOK4VFNf26jl9/f8+fP6/MzExdd911kqQdO3YUq+u+++5z/Nvd3V2RkZGyLEujRo1ytPv7+xc73hUrVqhjx47q0KGDU+29evWSpGJzGBcXp9atWzu+7tq1q/z8/Co0h0BlcFoKKMNLL72kdu3aKSsrSy+//LI2btwob29vx/PfffedLMvSE088oSeeeKLEMU6ePKnQ0FA9+eSTuuOOO9SuXTt16dJFt956q4YNG6auXbu6/Dg2bdqk6dOnKzU1Vbm5uU7PZWVlyW63l7l9ZWv/4YcfFB0dXay9ffv2Tl8XhaaiN8nf8vPzK3M/5bF//35lZWUpKCioxOdPnjzp9HWzZs2K9WnQoEGx63PK0rJlyxLr+Oqrr0o99XKpOoqCzq/rOH36tJKSkrR8+fJi25cUXn87pt1ul4+Pj+O00K/bf33dzv79+7V79+5K115Uf0XmEKgMwg1QhqioKMfdUgkJCbrhhht0zz33aO/evapXr57jAtCJEycW+w29SJs2bSRJN910kw4cOKB3331XH330kf75z39q9uzZWrhwoeM3aZvNJsuyio1RUFBQ6WM4cOCAevfurQ4dOujFF19UWFiYvLy8tHr1as2ePbvUi3t/rTy1X46iGl599VWFhIQUe97D4/J/VBUWFiooKEhLly4t8fnfvmG7u7uX2K+k709pSrq2qLCwULfccosmT55c4jbt2rWrcB2DBg3S5s2bNWnSJHXv3t3x2rz11ltL/P6WNGZ59lNYWKjw8HC9+OKLJfYNCwur8JiAKxBugHJyd3dXcnKybr75Zs2fP19TpkxRq1atJEmenp6Ki4u75BgBAQEaOXKkRo4cqZycHN10002aMWOGIyA0aNCgxCX7H3744ZJj//ai0SLvvfee8vLytGrVKqffpEs61VPaGOWpvSTNmzcv8VTW3r17nb4uOnURFBRUrnmsjNatW+vjjz9Wjx49qux27LLmq6w6cnJyquw4f/rpJ6WkpCgpKUnTpk1ztLvi9Gfr1q21a9cu9e7du1LHXpKqGgf4Na65ASqgZ8+eioqK0pw5c3T+/HkFBQWpZ8+e+vvf/64TJ04U63/q1CnHv397W269evXUpk0bp1toW7durT179jhtt2vXLm3atOmStdWtW1eSin0YWtFvz7/+bTkrK0uLFy8ucYySPkytPLWXpG/fvvr888/1xRdfONpOnTpVbPUkPj5efn5+evrpp5Wfn19snF/PR2UNGjRIBQUFeuqpp4o9d/HixUp9iFxpc36pOlJTU/Xhhx8We+7MmTO6ePFihWoo6fsrqdhdV1Vh0KBBOnbsmP7xj38Ue+7nn3/WuXPnKjxmZeYQuBRWboAKmjRpkn7/+99ryZIleuCBB/TSSy/phhtuUHh4uEaPHq1WrVopIyNDqampOnr0qHbt2iXplwtUe/bsqYiICAUEBGjbtm1auXKlxo0b5xj73nvv1Ysvvqj4+HiNGjVKJ0+e1MKFC9W5c2dlZ2eXWVdERIQk6bHHHtPdd98tT09P9e/fX3369JGXl5f69++vMWPGKCcnR//4xz8UFBRULJBFRERowYIFmjlzptq0aaOgoCD16tWrXLWXZPLkyXr11Vd166236uGHH3bcCt68eXN99dVXjn5+fn5asGCBhg0bpmuuuUZ33323AgMDdfjwYX3wwQfq0aOH5s+fX6Hv02/FxsZqzJgxSk5O1pdffqk+ffrI09NT+/fv14oVKzR37lwNHDiwQmN2795d7u7umjVrlrKysuTt7e34PKHSTJo0SatWrdLtt9/uuLX83Llz+vrrr7Vy5UodOnSo2LUvZfHz89NNN92kZ599Vvn5+QoNDdVHH32kgwcPVuhYymPYsGF688039cADD2j9+vXq0aOHCgoKtGfPHr355puOz/SpiNJet0WhB6iUGrtPC7iCFd0KvnXr1mLPFRQUWK1bt7Zat25tXbx40bIsyzpw4IA1fPhwKyQkxPL09LRCQ0Ot22+/3Vq5cqVju5kzZ1pRUVGWv7+/5evra3Xo0MH6y1/+Yl24cMFp/Ndee81q1aqV5eXlZXXv3t368MMPy3UruGX9cptxaGio5ebm5nR77apVq6yuXbtaPj4+VosWLaxZs2ZZL7/8crFbcNPT061+/fpZ9evXtyQ5bgsvb+0l+eqrr6zY2FjLx8fHCg0NtZ566inrX//6V4m3/65fv96Kj4+37Ha75ePjY7Vu3dpKTEy0tm3bVuY+ynMreJFFixZZERERlq+vr1W/fn0rPDzcmjx5snX8+HFHn+bNm5d463tJt+r/4x//sFq1amW5u7s71VDaGJb1y63vU6dOtdq0aWN5eXlZjRo1sq6//nrr+eefd8xp0a3gzz33XLHtf/u9P3r0qHXnnXda/v7+lt1ut37/+99bx48fL9av6FbwU6dOOY03YsQIq27duiUeb+fOnZ3aLly4YM2aNcvq3Lmz5e3tbTVo0MCKiIiwkpKSrKysLKcax44dW2zM5s2bO91eblmlv26ByrJZFld2AQAAc3DNDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUa7KD/ErLCzU8ePHVb9+fT76GwCAWsKyLJ09e1ZNmjSRm1vp6zNXZbg5fvx4sT/wBgAAaocjR46oadOmpT5/VYab+vXrS/plcvz8/Gq4GgAAUB7Z2dkKCwtzvI+X5qoMN0Wnovz8/Ag3AADUMpe6pIQLigEAgFEINwAAwCiEGwAAYJSr8pobAABcpaCgQPn5+TVdRq3k7u4uDw+Py/6YFsINAABVJCcnR0ePHpVlWTVdSq1Vp04dNW7cWF5eXpUeg3ADAEAVKCgo0NGjR1WnTh0FBgbyIbEVZFmWLly4oFOnTungwYNq27ZtmR/UVxbCDQAAVSA/P1+WZSkwMFC+vr41XU6t5OvrK09PT/3www+6cOGCfHx8KjUOFxQDAFCFWLG5PJVdrXEaowrqAAAAuGIQbgAAgFEINwAAoEq0aNFCc+bMqekyuKAYAICrWc+ePdW9e/cqCSVbt25V3bp1L7+oy0S4AQAApbIsSwUFBfLwuHRkCAwMrIaKLo3TUgAAuIJlSefO1cyjnB8imJiYqA0bNmju3Lmy2Wyy2WxasmSJbDab/vvf/yoiIkLe3t767LPPdODAAd1xxx0KDg5WvXr1dO211+rjjz92Gu+3p6VsNpv++c9/6s4771SdOnXUtm1brVq1qipnuUSEGwAAXCE3V6pXr2YeubnlKnHu3LmKiYnR6NGjdeLECZ04cUJhYWGSpClTpuiZZ57R7t271bVrV+Xk5Khv375KSUnRzp07deutt6p///46fPhwmftISkrSoEGD9NVXX6lv374aOnSoTp8+fdnTWxbCDQAAVym73S4vLy/VqVNHISEhCgkJkbu7uyTpySef1C233KLWrVsrICBA3bp105gxY9SlSxe1bdtWTz31lFq3bn3JlZjExEQNGTJEbdq00dNPP62cnBx98cUXLj0urrkBAMAV6tSRcnJqbt+XKTIy0unrnJwczZgxQx988IFOnDihixcv6ueff77kyk3Xrl0d/65bt678/Px08uTJy66vLIQbAABcwWaTroA7hyrrt3c9TZw4UWvXrtXzzz+vNm3ayNfXVwMHDtSFCxfKHMfT09Ppa5vNpsLCwiqv99cINwAAXMW8vLxUUFBwyX6bNm1SYmKi7rzzTkm/rOQcOnTIxdVVDtfcAABwFWvRooW2bNmiQ4cOKTMzs9RVlbZt2+qtt97Sl19+qV27dumee+5x+QpMZRFuAAC4ik2cOFHu7u7q1KmTAgMDS72G5sUXX1SDBg10/fXXq3///oqPj9c111xTzdWWj82yynkzvEGys7Nlt9uVlZUlPz+/mi4HAGCA8+fP6+DBg2rZsqV8fHxqupxaq6x5LO/7Nys3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AAFexnj17avz48VU2XmJiohISEqpsvMog3AAAAKMQbgAAcAHLsnTuwrkaeZT3b2InJiZqw4YNmjt3rmw2m2w2mw4dOqRvvvlGt912m+rVq6fg4GANGzZMmZmZju1Wrlyp8PBw+fr6qmHDhoqLi9O5c+c0Y8YM/fvf/9a7777rGO+TTz5x0QyXzqPa9wgAwFUgNz9X9ZLr1ci+c6bmqK5X3Uv2mzt3rvbt26cuXbroySeflCR5enoqKipK9913n2bPnq2ff/5Zf/7znzVo0CCtW7dOJ06c0JAhQ/Tss8/qzjvv1NmzZ/Xpp5/KsixNnDhRu3fvVnZ2thYvXixJCggIcOmxloRwAwDAVcput8vLy0t16tRRSEiIJGnmzJn6n//5Hz399NOOfi+//LLCwsK0b98+5eTk6OLFi7rrrrvUvHlzSVJ4eLijr6+vr/Ly8hzj1QTCDQAALlDHs45ypubU2L4ra9euXVq/fr3q1Su+6nTgwAH16dNHvXv3Vnh4uOLj49WnTx8NHDhQDRo0uJySqxThBgAAF7DZbOU6NXSlycnJUf/+/TVr1qxizzVu3Fju7u5au3atNm/erI8++kh//etf9dhjj2nLli1q2bJlDVRcHBcUAwBwFfPy8lJBQYHj62uuuUZpaWlq0aKF2rRp4/SoW/eXsGaz2dSjRw8lJSVp586d8vLy0ttvv13ieDWBcAMAwFWsRYsW2rJliw4dOqTMzEyNHTtWp0+f1pAhQ7R161YdOHBAH374oUaOHKmCggJt2bJFTz/9tLZt26bDhw/rrbfe0qlTp9SxY0fHeF999ZX27t2rzMxM5efnV/sxEW4AALiKTZw4Ue7u7urUqZMCAwN14cIFbdq0SQUFBerTp4/Cw8M1fvx4+fv7y83NTX5+ftq4caP69u2rdu3a6fHHH9cLL7yg2267TZI0evRotW/fXpGRkQoMDNSmTZuq/ZhsVnlvhjdIdna27Ha7srKy5OfnV9PlAAAMcP78eR08eFAtW7aUj49PTZdTa5U1j+V9/66WlZuXXnpJLVq0kI+Pj6Kjo/XFF1+U2X/FihXq0KGDfHx8FB4ertWrV5fa94EHHpDNZtOcOXOquGoAAFAbuTzcvPHGG5owYYKmT5+uHTt2qFu3boqPj9fJkydL7L9582YNGTJEo0aN0s6dO5WQkKCEhAR98803xfq+/fbb+vzzz9WkSRNXHwYAAKglXB5uXnzxRY0ePVojR45Up06dtHDhQtWpU0cvv/xyif3nzp2rW2+9VZMmTVLHjh311FNP6ZprrtH8+fOd+h07dkwPPvigli5dKk9PT1cfBgAAqCVcGm4uXLig7du3Ky4u7v926OamuLg4paamlrhNamqqU39Jio+Pd+pfWFioYcOGadKkSercufMl68jLy1N2drbTAwAAmMml4SYzM1MFBQUKDg52ag8ODlZ6enqJ26Snp1+y/6xZs+Th4aGHHnqoXHUkJyfLbrc7HmFhYRU8EgAAyucqvE+nSlXF/NW6W8G3b9+uuXPnasmSJbLZbOXaZurUqcrKynI8jhw54uIqAQBXG3d3d0m/nLVA5eXm5krSZV1y4tI/v9CoUSO5u7srIyPDqT0jI6PUP6gVEhJSZv9PP/1UJ0+eVLNmzRzPFxQU6JFHHtGcOXN06NChYmN6e3vL29v7Mo8GAIDSeXh4qE6dOjp16pQ8PT3l5lbr1g9qlGVZys3N1cmTJ+Xv7+8Ii5Xh0nDj5eWliIgIpaSkKCEhQdIv18ukpKRo3LhxJW4TExOjlJQUjR8/3tG2du1axcTESJKGDRtW4jU5w4YN08iRI11yHAAAXIrNZlPjxo118OBB/fDDDzVdTq3l7+9/2X9R3OV/OHPChAkaMWKEIiMjFRUVpTlz5ujcuXOOIDJ8+HCFhoYqOTlZkvTwww8rNjZWL7zwgvr166fly5dr27ZtWrRokSSpYcOGatiwodM+PD09FRISovbt27v6cAAAKJWXl5fatm3LqalK8vT0vKwVmyIuDzeDBw/WqVOnNG3aNKWnp6t79+5as2aN46Lhw4cPOy3dXX/99Vq2bJkef/xxPfroo2rbtq3eeecddenSxdWlAgBw2dzc3PiE4hrGn1/gzy8AAFArXFF/fgEAAKC6EG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEaplnDz0ksvqUWLFvLx8VF0dLS++OKLMvuvWLFCHTp0kI+Pj8LDw7V69WrHc/n5+frzn/+s8PBw1a1bV02aNNHw4cN1/PhxVx8GAACoBVwebt544w1NmDBB06dP144dO9StWzfFx8fr5MmTJfbfvHmzhgwZolGjRmnnzp1KSEhQQkKCvvnmG0lSbm6uduzYoSeeeEI7duzQW2+9pb179+p3v/udqw8FAADUAjbLsixX7iA6OlrXXnut5s+fL0kqLCxUWFiYHnzwQU2ZMqVY/8GDB+vcuXN6//33HW3XXXedunfvroULF5a4j61btyoqKko//PCDmjVrdsmasrOzZbfblZWVJT8/v0oeGQAAqE7lff926crNhQsXtH37dsXFxf3fDt3cFBcXp9TU1BK3SU1NdeovSfHx8aX2l6SsrCzZbDb5+/uX+HxeXp6ys7OdHgAAwEwuDTeZmZkqKChQcHCwU3twcLDS09NL3CY9Pb1C/c+fP68///nPGjJkSKkpLjk5WXa73fEICwurxNEAAIDaoFbfLZWfn69BgwbJsiwtWLCg1H5Tp05VVlaW43HkyJFqrBIAAFQnD1cO3qhRI7m7uysjI8OpPSMjQyEhISVuExISUq7+RcHmhx9+0Lp168o89+bt7S1vb+9KHgUAAKhNXLpy4+XlpYiICKWkpDjaCgsLlZKSopiYmBK3iYmJceovSWvXrnXqXxRs9u/fr48//lgNGzZ0zQEAAIBax6UrN5I0YcIEjRgxQpGRkYqKitKcOXN07tw5jRw5UpI0fPhwhYaGKjk5WZL08MMPKzY2Vi+88IL69eun5cuXa9u2bVq0aJGkX4LNwIEDtWPHDr3//vsqKChwXI8TEBAgLy8vVx8SAAC4grk83AwePFinTp3StGnTlJ6eru7du2vNmjWOi4YPHz4sN7f/W0C6/vrrtWzZMj3++ON69NFH1bZtW73zzjvq0qWLJOnYsWNatWqVJKl79+5O+1q/fr169uzp6kMCAABXMJd/zs2ViM+5AQCg9rkiPucGAACguhFuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjVEu4eemll9SiRQv5+PgoOjpaX3zxRZn9V6xYoQ4dOsjHx0fh4eFavXq10/OWZWnatGlq3LixfH19FRcXp/3797vyEAAAQC3h4eodvPHGG5owYYIWLlyo6OhozZkzR/Hx8dq7d6+CgoKK9d+8ebOGDBmi5ORk3X777Vq2bJkSEhK0Y8cOdenSRZL07LPPat68efr3v/+tli1b6oknnlB8fLy+/fZb+fj4uPqQSmUVFio3K7PG9g8AwJWijr2RbG41c4LIZlmW5codREdH69prr9X8+fMlSYWFhQoLC9ODDz6oKVOmFOs/ePBgnTt3Tu+//76j7brrrlP37t21cOFCWZalJk2a6JFHHtHEiRMlSVlZWQoODtaSJUt09913FxszLy9PeXl5jq+zs7MVFhamrKws+fn5VdmxnvvppOrNC66y8QAAqK1yHspQ3QbFFzEuR3Z2tux2+yXfv10aqS5cuKDt27crLi7u/3bo5qa4uDilpqaWuE1qaqpTf0mKj4939D948KDS09Od+tjtdkVHR5c6ZnJysux2u+MRFhZ2uYcGAACuUC49LZWZmamCggIFBzuvZgQHB2vPnj0lbpOenl5i//T0dMfzRW2l9fmtqVOnasKECY6vi1ZuqlodeyPlPJRR5eMCAFDb1LE3qrF9u/yamyuBt7e3vL29Xb4fm5tblS/BAQCAinHpaalGjRrJ3d1dGRnOqxkZGRkKCQkpcZuQkJAy+xf9tyJjAgCAq4dLw42Xl5ciIiKUkpLiaCssLFRKSopiYmJK3CYmJsapvyStXbvW0b9ly5YKCQlx6pOdna0tW7aUOiYAALh6uPy01IQJEzRixAhFRkYqKipKc+bM0blz5zRy5EhJ0vDhwxUaGqrk5GRJ0sMPP6zY2Fi98MIL6tevn5YvX65t27Zp0aJFkiSbzabx48dr5syZatu2reNW8CZNmighIcHVhwMAAK5wLg83gwcP1qlTpzRt2jSlp6ere/fuWrNmjeOC4MOHD8vtV/fBX3/99Vq2bJkef/xxPfroo2rbtq3eeecdx2fcSNLkyZN17tw53X///Tpz5oxuuOEGrVmzpkY/4wYAAFwZXP45N1ei8t4nDwAArhxXxOfcAAAAVDfCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKC4LN6dPn9bQoUPl5+cnf39/jRo1Sjk5OWVuc/78eY0dO1YNGzZUvXr1NGDAAGVkZDie37Vrl4YMGaKwsDD5+vqqY8eOmjt3rqsOAQAA1EIuCzdDhw5VWlqa1q5dq/fff18bN27U/fffX+Y2f/rTn/Tee+9pxYoV2rBhg44fP6677rrL8fz27dsVFBSk1157TWlpaXrsscc0depUzZ8/31WHAQAAahmbZVlWVQ+6e/duderUSVu3blVkZKQkac2aNerbt6+OHj2qJk2aFNsmKytLgYGBWrZsmQYOHChJ2rNnjzp27KjU1FRdd911Je5r7Nix2r17t9atW1fu+rKzs2W325WVlSU/P79KHCEAAKhu5X3/dsnKTWpqqvz9/R3BRpLi4uLk5uamLVu2lLjN9u3blZ+fr7i4OEdbhw4d1KxZM6Wmppa6r6ysLAUEBJRZT15enrKzs50eAADATC4JN+np6QoKCnJq8/DwUEBAgNLT00vdxsvLS/7+/k7twcHBpW6zefNmvfHGG5c83ZWcnCy73e54hIWFlf9gAABArVKhcDNlyhTZbLYyH3v27HFVrU6++eYb3XHHHZo+fbr69OlTZt+pU6cqKyvL8Thy5Ei11AgAAKqfR0U6P/LII0pMTCyzT6tWrRQSEqKTJ086tV+8eFGnT59WSEhIiduFhITowoULOnPmjNPqTUZGRrFtvv32W/Xu3Vv333+/Hn/88UvW7e3tLW9v70v2AwAAtV+Fwk1gYKACAwMv2S8mJkZnzpzR9u3bFRERIUlat26dCgsLFR0dXeI2ERER8vT0VEpKigYMGCBJ2rt3rw4fPqyYmBhHv7S0NPXq1UsjRozQX/7yl4qUDwAArgIuuVtKkm677TZlZGRo4cKFys/P18iRIxUZGally5ZJko4dO6bevXvrlVdeUVRUlCTpf//3f7V69WotWbJEfn5+evDBByX9cm2N9MupqF69eik+Pl7PPfecY1/u7u7lCl1FuFsKAIDap7zv3xVauamIpUuXaty4cerdu7fc3Nw0YMAAzZs3z/F8fn6+9u7dq9zcXEfb7NmzHX3z8vIUHx+vv/3tb47nV65cqVOnTum1117Ta6+95mhv3ry5Dh065KpDAQAAtYjLVm6uZKzcAABQ+9To59wAAADUFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAoLgs3p0+f1tChQ+Xn5yd/f3+NGjVKOTk5ZW5z/vx5jR07Vg0bNlS9evU0YMAAZWRklNj3xx9/VNOmTWWz2XTmzBkXHAEAAKiNXBZuhg4dqrS0NK1du1bvv/++Nm7cqPvvv7/Mbf70pz/pvffe04oVK7RhwwYdP35cd911V4l9R40apa5du7qidAAAUIvZLMuyqnrQ3bt3q1OnTtq6dasiIyMlSWvWrFHfvn119OhRNWnSpNg2WVlZCgwM1LJlyzRw4EBJ0p49e9SxY0elpqbquuuuc/RdsGCB3njjDU2bNk29e/fWTz/9JH9//3LXl52dLbvdrqysLPn5+V3ewQIAgGpR3vdvl6zcpKamyt/f3xFsJCkuLk5ubm7asmVLidts375d+fn5iouLc7R16NBBzZo1U2pqqqPt22+/1ZNPPqlXXnlFbm7lKz8vL0/Z2dlODwAAYCaXhJv09HQFBQU5tXl4eCggIEDp6emlbuPl5VVsBSY4ONixTV5enoYMGaLnnntOzZo1K3c9ycnJstvtjkdYWFjFDggAANQaFQo3U6ZMkc1mK/OxZ88eV9WqqVOnqmPHjvrDH/5Q4e2ysrIcjyNHjrioQgAAUNM8KtL5kUceUWJiYpl9WrVqpZCQEJ08edKp/eLFizp9+rRCQkJK3C4kJEQXLlzQmTNnnFZvMjIyHNusW7dOX3/9tVauXClJKrpcqFGjRnrssceUlJRU4tje3t7y9vYuzyECAIBarkLhJjAwUIGBgZfsFxMTozNnzmj79u2KiIiQ9EswKSwsVHR0dInbREREyNPTUykpKRowYIAkae/evTp8+LBiYmIkSf/5z3/0888/O7bZunWr7r33Xn366adq3bp1RQ4FAAAYqkLhprw6duyoW2+9VaNHj9bChQuVn5+vcePG6e6773bcKXXs2DH17t1br7zyiqKiomS32zVq1ChNmDBBAQEB8vPz04MPPqiYmBjHnVK/DTCZmZmO/VXkbikAAGAul4QbSVq6dKnGjRun3r17y83NTQMGDNC8efMcz+fn52vv3r3Kzc11tM2ePdvRNy8vT/Hx8frb3/7mqhIBAICBXPI5N1c6PucGAIDap0Y/5wYAAKCmEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUTxquoCaYFmWJCk7O7uGKwEAAOVV9L5d9D5emqsy3Jw9e1aSFBYWVsOVAACAijp79qzsdnupz9usS8UfAxUWFur48eOqX7++bDZblY6dnZ2tsLAwHTlyRH5+flU6Nv4P81w9mOfqwTxXD+a5erhyni3L0tmzZ9WkSRO5uZV+Zc1VuXLj5uampk2bunQffn5+/M9TDZjn6sE8Vw/muXowz9XDVfNc1opNES4oBgAARiHcAAAAoxBuqpi3t7emT58ub2/vmi7FaMxz9WCeqwfzXD2Y5+pxJczzVXlBMQAAMBcrNwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4qYCNGzeqf//+atKkiWw2m955551LbvPJJ5/ommuukbe3t9q0aaMlS5a4vM7arqLz/NZbb+mWW25RYGCg/Pz8FBMTow8//LB6iq3FKvN6LrJp0yZ5eHioe/fuLqvPJJWZ67y8PD322GNq3ry5vL291aJFC7388suuL7YWq8w8L126VN26dVOdOnXUuHFj3Xvvvfrxxx9dX2wtlZycrGuvvVb169dXUFCQEhIStHfv3ktut2LFCnXo0EE+Pj4KDw/X6tWrXVon4aYCzp07p27duumll14qV/+DBw+qX79+uvnmm/Xll19q/Pjxuu+++3jjvYSKzvPGjRt1yy23aPXq1dq+fbtuvvlm9e/fXzt37nRxpbVbRee5yJkzZzR8+HD17t3bRZWZpzJzPWjQIKWkpOhf//qX9u7dq9dff13t27d3YZW1X0XnedOmTRo+fLhGjRqltLQ0rVixQl988YVGjx7t4kprrw0bNmjs2LH6/PPPtXbtWuXn56tPnz46d+5cqdts3rxZQ4YM0ahRo7Rz504lJCQoISFB33zzjesKtVApkqy33367zD6TJ0+2Onfu7NQ2ePBgKz4+3oWVmaU881ySTp06WUlJSVVfkKEqMs+DBw+2Hn/8cWv69OlWt27dXFqXicoz1//9738tu91u/fjjj9VTlIHKM8/PPfec1apVK6e2efPmWaGhoS6szCwnT560JFkbNmwotc+gQYOsfv36ObVFR0dbY8aMcVldrNy4UGpqquLi4pza4uPjlZqaWkMVXR0KCwt19uxZBQQE1HQpxlm8eLG+//57TZ8+vaZLMdqqVasUGRmpZ599VqGhoWrXrp0mTpyon3/+uaZLM0pMTIyOHDmi1atXy7IsZWRkaOXKlerbt29Nl1ZrZGVlSVKZP29r4r3wqvyr4NUlPT1dwcHBTm3BwcHKzs7Wzz//LF9f3xqqzGzPP/+8cnJyNGjQoJouxSj79+/XlClT9Omnn8rDgx8drvT999/rs88+k4+Pj95++21lZmbqj3/8o3788UctXry4psszRo8ePbR06VINHjxY58+f18WLF9W/f/8Kn6q9WhUWFmr8+PHq0aOHunTpUmq/0t4L09PTXVYbKzcwyrJly5SUlKQ333xTQUFBNV2OMQoKCnTPPfcoKSlJ7dq1q+lyjFdYWCibzaalS5cqKipKffv21Ysvvqh///vfrN5UoW+//VYPP/ywpk2bpu3bt2vNmjU6dOiQHnjggZourVYYO3asvvnmGy1fvrymSymGX79cKCQkRBkZGU5tGRkZ8vPzY9XGBZYvX6777rtPK1asKLYEistz9uxZbdu2TTt37tS4ceMk/fIGbFmWPDw89NFHH6lXr141XKU5GjdurNDQUNntdkdbx44dZVmWjh49qrZt29ZgdeZITk5Wjx49NGnSJElS165dVbduXd14442aOXOmGjduXMMVXrnGjRun999/Xxs3blTTpk3L7Fvae2FISIjL6mPlxoViYmKUkpLi1LZ27VrFxMTUUEXmev311zVy5Ei9/vrr6tevX02XYxw/Pz99/fXX+vLLLx2PBx54QO3bt9eXX36p6Ojomi7RKD169NDx48eVk5PjaNu3b5/c3Nwu+UaC8svNzZWbm/PboLu7uyTJ4m9Kl8iyLI0bN05vv/221q1bp5YtW15ym5p4L2TlpgJycnL03XffOb4+ePCgvvzySwUEBKhZs2aaOnWqjh07pldeeUWS9MADD2j+/PmaPHmy7r33Xq1bt05vvvmmPvjgg5o6hFqhovO8bNkyjRgxQnPnzlV0dLTjPK6vr6/Tb75wVpF5dnNzK3ZOPSgoSD4+PmWea8cvKvqavueee/TUU09p5MiRSkpKUmZmpiZNmqR7772XVd8yVHSe+/fvr9GjR2vBggWKj4/XiRMnNH78eEVFRalJkyY1dRhXtLFjx2rZsmV69913Vb9+fcfPW7vd7nhtDh8+XKGhoUpOTpYkPfzww4qNjdULL7ygfv36afny5dq2bZsWLVrkukJddh+WgdavX29JKvYYMWKEZVmWNWLECCs2NrbYNt27d7e8vLysVq1aWYsXL672umubis5zbGxsmf1Rssq8nn+NW8HLrzJzvXv3bisuLs7y9fW1mjZtak2YMMHKzc2t/uJrkcrM87x586xOnTpZvr6+VuPGja2hQ4daR48erf7ia4mS5leS03tbbGxssZ+/b775ptWuXTvLy8vL6ty5s/XBBx+4tE7b/y8WAADACFxzAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACj/D9D7mkACc0JfwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Resultats de l'entrenament\")\n",
    "plt.plot(range(1, (epochs + 1)), train_l,  c=\"red\", label=\"train\")\n",
    "plt.plot(range(1,  (epochs + 1)), test_l,  c=\"green\", label=\"test\")\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
